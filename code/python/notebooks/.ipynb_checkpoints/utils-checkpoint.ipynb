{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f59374-92d0-42b1-955c-cc08949b40bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.vectorized import contains\n",
    "import subprocess\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110cd3b4-352b-47d7-b146-f053d43037f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: rename variables so that datetime objects and strings are recognisable\n",
    "\n",
    "def get_irradiance_dataset(dir_dt):\n",
    "    utc_dt = dir_dt - timedelta(hours=5, minutes=30)\n",
    "    file_dt = utc_dt.strftime(\"%Y%m%d%H%M\")\n",
    "    filename='IDE00326.'+file_dt+'.nc'\n",
    "\n",
    "    if utc_dt < datetime.strptime('2019-03-31', '%Y-%m-%d'):\n",
    "        file = 'v1.0'\n",
    "    else:\n",
    "        file = 'v1.1'\n",
    "    \n",
    "    dirin=f'/g/data/rv74/satellite-products/arc/der/himawari-ahi/solar/p1s/{file}/'+f\"{dir_dt.year:04}\"+'/'+f\"{dir_dt.month:02}\"+'/'+f\"{dir_dt.day:02}\"+\"/\"\n",
    "    try:\n",
    "        dataset = Dataset(dirin+filename)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {dirin + filename}.\")\n",
    "        dataset = None\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f59c5b-521d-4a6d-a64b-361a2f99d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(area_bounds=None):\n",
    "    \n",
    "    # same lat/lon in all files, pick any to get coordinate data\n",
    "    dataset = get_irradiance_dataset(datetime.strptime('1-1-2023', \"%d-%m-%Y\"))\n",
    "    \n",
    "    latitudes=dataset.variables['latitude'][:]\n",
    "    longitudes=dataset.variables['longitude'][:]\n",
    "    if area_bounds == None:\n",
    "        return latitudes, longitudes\n",
    "    lat_indices=np.where((dataset.variables['latitude'][:] >= area_bounds[\"lat_min\"]) & (dataset.variables['latitude'][:] <= area_bounds[\"lat_max\"]))[0]\n",
    "    lon_indices=np.where((dataset.variables['longitude'][:] >= area_bounds[\"lon_min\"]) & (dataset.variables['longitude'][:] <= area_bounds[\"lon_max\"]))[0]\n",
    "    \n",
    "    return latitudes, longitudes, lat_indices, lon_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffe24695-9c75-4f8e-8e6d-f795d0e96379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_irradiance(start_date, end_date, area_bounds=None, region_masks=None):\n",
    "\n",
    "    '''\n",
    "    INPUTS\n",
    "    num_days: number of days of data to inspect\n",
    "\n",
    "    start_df: first date to inspect\n",
    "\n",
    "    area_bounds: dictionary containing min and max lat/lon coordinates\n",
    "    '''\n",
    "\n",
    "    # datetime object for finding files\n",
    "    dir_dt = datetime.strptime(start_date, \"%d-%m-%Y\")\n",
    "    end_dt = datetime.strptime(end_date, \"%d-%m-%Y\")\n",
    "    num_days = (end_dt - dir_dt).days+1\n",
    "\n",
    "    # coordinates to inspect\n",
    "    if area_bounds:\n",
    "        latitudes, longitudes, lat_indices, lon_indices = get_coords(area_bounds=area_bounds)\n",
    "\n",
    "    # dict for DF\n",
    "    rad_data = {'date':[], 'daily_mean':[]}\n",
    "\n",
    "    # improvement for later, remove hard coded value for file_count\n",
    "    file_count = 103 # number of files in each directory\n",
    "    for i in range(file_count*num_days):\n",
    "        dataset = get_irradiance_dataset(dir_dt=dir_dt)\n",
    "\n",
    "        # first file of the day, add the date of that day to the dict\n",
    "        if dir_dt.strftime('%H%M') == '0000':\n",
    "            rad_data['date'].append(f'{dir_dt.day}-{dir_dt.month}-{dir_dt.year}')\n",
    "            daily_data = []\n",
    "\n",
    "        if dataset is not None:\n",
    "            # only look at data if file exists\n",
    "            if area_bounds is not None:\n",
    "                # Extract and squeeze irradiance data for the area\n",
    "                irradiance = np.squeeze(dataset.variables['surface_global_irradiance'][:, lat_indices, :][:, :, lon_indices])\n",
    "                daily_data.append(irradiance)\n",
    "                \n",
    "            elif region_masks is not None:\n",
    "                irradiance = np.squeeze(dataset.variables['surface_global_irradiance'][:,:,:][:,:,:])\n",
    "                region_means = np.ma.masked_array(np.repeat(irradiance[None, ...], len(region_masks), axis=0), mask=~region_masks).mean(axis=(1, 2))\n",
    "                daily_data.append(region_means)\n",
    "            else:\n",
    "                irradiance = np.squeeze(dataset.variables['surface_global_irradiance'][:,:,:][:,:,:])\n",
    "                daily_data.append(irradiance)\n",
    "                \n",
    "            dataset.close()\n",
    "\n",
    "        if dir_dt.strftime('%H%M') == '1700': # last file of day\n",
    "            dir_dt = dir_dt + timedelta(hours=7)\n",
    "            daily_mean = np.ma.mean(np.ma.stack(daily_data), axis=0)\n",
    "            rad_data['daily_mean'].append(daily_mean)\n",
    "            \n",
    "        else:\n",
    "            dir_dt = dir_dt + timedelta(minutes=10)\n",
    "    return pd.DataFrame(rad_data).set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8915213f-0eaa-492a-88fa-7210687b3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_area(data, fig_name, area_bounds=None, vmax=None):\n",
    "\n",
    "    if area_bounds is not None:\n",
    "        latitudes, longitudes, lat_indices, lon_indices = get_coords(area_bounds=area_bounds)\n",
    "        lon, lat = np.meshgrid(longitudes[lon_indices], latitudes[lat_indices])\n",
    "    else:\n",
    "        lat, lon = get_coords()\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    \n",
    "    ax.coastlines()\n",
    "    \n",
    "    # Plot the 2D masked array using pcolormesh with Cartopy CRS\n",
    "    mesh = ax.pcolormesh(lon, lat, data, cmap='viridis', vmin=0, vmax=vmax, shading='auto', transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # Add a colorbar\n",
    "    plt.colorbar(mesh, ax=ax, label='Num. Days / season', shrink=0.5)\n",
    "    \n",
    "    # Set plot title and labels\n",
    "    ax.set_title(fig_name)\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'/home/548/cd3022/aus-historical-solar-droughts/figs/heatmaps/{fig_name}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a163b723-7331-40c8-bacf-92bd95b5ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_mask(shape_file, regions):\n",
    "\n",
    "    # https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files\n",
    "    gccsa_file = shape_file\n",
    "    gccsa = gpd.read_file(gccsa_file, encoding='utf-8')\n",
    "    gccsa = gccsa.to_crs(\"EPSG:4326\")\n",
    "    all_regions = gccsa[gccsa['GCC_CODE21'].isin(regions)]\n",
    "\n",
    "    # lat and lon values from datasets\n",
    "    latitudes, longitudes = get_coords()\n",
    "    # Create a meshgrid of latitudes and longitudes\n",
    "    lon_grid, lat_grid = np.meshgrid(longitudes, latitudes)\n",
    "    # Flatten the grids into 1D arrays\n",
    "    lon_flat = lon_grid.ravel()\n",
    "    lat_flat = lat_grid.ravel()\n",
    "    \n",
    "    # Prepare the mask for all regions at once\n",
    "    region_masks = np.array([\n",
    "        contains(gccsa[gccsa['GCC_CODE21'] == city].unary_union, lon_flat, lat_flat).reshape(lon_grid.shape)\n",
    "        for city in regions\n",
    "    ])\n",
    "    return region_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5607d97-904f-4623-a82a-09a758f518ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_droughts(data, definition, threshold):\n",
    "    '''\n",
    "    df: DataFrame contain mean daily values as 2D masked arrays\n",
    "\n",
    "    definition: how droughts are being defined\n",
    "\n",
    "    threshold: cut-off for identifying droughts\n",
    "    '''\n",
    "\n",
    "    # Find baseline conditions (depending on definition) for national and regional data\n",
    "    if definition == 'mean':\n",
    "        total = np.ma.mean(np.ma.stack(data), axis=0)\n",
    "    elif definition == 'max':\n",
    "        total = np.ma.max(np.ma.stack(data), axis=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid definition '{definition}'. Please provide 'mean' or 'max'.\")\n",
    "        \n",
    "    # total.mask = total < 10 # remove edge values with small mean that have significantly more drought days\n",
    "        \n",
    "    # Produce list of 2D arrays with bool values to indicate if data point is a drought\n",
    "    is_drought_day = [\n",
    "        np.ma.masked_array(\n",
    "            dm < (total * threshold),\n",
    "            mask = dm < 10 \n",
    "        )\n",
    "        for dm in data\n",
    "    ]\n",
    "  \n",
    "    # count the number of droughts occuring at the same time\n",
    "    coincident_droughts = [np.sum(droughts) for droughts in is_drought_day]\n",
    "    \n",
    "    return pd.DataFrame({'is_drought_day': is_drought_day, 'coincident_droughts': coincident_droughts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d072c4-b518-4b29-82d9-b09c855525a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regional_drought_lengths(df, regions):\n",
    "\n",
    "# Find consecutive droughts days, count length of drought and number of unique droughts\n",
    "\n",
    "# makes a dict of regional dicts\n",
    "# each regional dict contains a dict for how many times that region experienced a drought of different legnths\n",
    "\n",
    "# Generate 2D array same as \"is_drought_day\", but cumulative days increment by 1\n",
    "# Create an array for cumulative drought counts\n",
    "    drought_stack = np.stack(df['is_drought_day'].values)\n",
    "    cumulative_droughts = np.zeros_like(drought_stack, dtype=int)\n",
    "    \n",
    "    \n",
    "    # Iterate through the time axis while resetting counts on drought breaks\n",
    "    cumulative_droughts[0] = drought_stack[0]  # Initialize the first time step\n",
    "    for t in range(1, drought_stack.shape[0]):\n",
    "        # Increment drought count where drought continues\n",
    "        cumulative_droughts[t] = (\n",
    "            (cumulative_droughts[t - 1] + 1) * drought_stack[t]\n",
    "        )\n",
    "\n",
    "    # Make dict of drought lengths, and count how many droughts of each length there are\n",
    "    regional_drought_lengths = {}\n",
    "    for i, region in enumerate(regions):\n",
    "        # Make dict of possible drought lengths up to 7 days\n",
    "        drought_lengths = {length: 0 for length in [str(i) for i in range(1, 8)]}\n",
    "\n",
    "        # count droughts\n",
    "        for j in range(len(cumulative_droughts[:, i])-1):\n",
    "            if cumulative_droughts[j, i] == 0:\n",
    "                continue\n",
    "            if cumulative_droughts[j+1, i] != 0:\n",
    "                continue\n",
    "            length = cumulative_droughts[j, i]\n",
    "            drought_lengths[f'{length}'] += 1\n",
    "            \n",
    "        if cumulative_droughts[-1, i] != 0:\n",
    "            length = cumulative_droughts[-1, i]\n",
    "            if f'{length}' not in drought_lengths:\n",
    "                drought_lengths[f'{length}'] = 1\n",
    "        regional_drought_lengths[region] = drought_lengths\n",
    "    return regional_drought_lengths, cumulative_droughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138146c1-ab3a-4815-b957-949ce0b44e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/548/cd3022/aus-historical-solar-droughts/code/python/notebooks/utils.ipynb to script\n",
      "[NbConvertApp] Writing 9745 bytes to /home/548/cd3022/aus-historical-solar-droughts/code/python/scripts/utils.py\n",
      "name == main\n"
     ]
    }
   ],
   "source": [
    "# Convert to .py file to be imported by other modules\n",
    "if __name__ == '__main__':\n",
    "    !jupyter nbconvert --to script \"/home/548/cd3022/aus-historical-solar-droughts/code/python/notebooks/utils.ipynb\" --output \"/home/548/cd3022/aus-historical-solar-droughts/code/python/scripts/utils\"\n",
    "    print('name == main')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
